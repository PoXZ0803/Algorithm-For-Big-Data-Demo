{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Spark\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Using Apache Spark with Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Map-Reduce on a cluster of computers\n",
    "\n",
    "- The code we have written so far will *not* allow us to exploit parallelism from multiple computers in a [cluster](https://en.wikipedia.org/wiki/Computer_cluster).\n",
    "\n",
    "- Developing such a framework would be a very large software engineering project.\n",
    "\n",
    "- There are existing frameworks we can use:\n",
    "    - [Apache Hadoop](https://hadoop.apache.org/)\n",
    "    - [Apache Spark](https://spark.apache.org/)\n",
    "    \n",
    "- In this lecture we will cover Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "- Apache Spark provides an object-oriented library for processing data on the cluster.\n",
    "\n",
    "- It provides objects which represent resilient distributed datasets (RDDs).\n",
    "\n",
    "- RDDs behave a bit like Python collections (e.g. lists).\n",
    "\n",
    "- However:\n",
    "    - the underlying data is distributed across the nodes in the cluster, and\n",
    "    - the collections are *immutable*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Apache Spark and Map-Reduce\n",
    "\n",
    "- We process the data by using higher-order functions to map RDDs onto *new* RDDs. \n",
    "\n",
    "- Each instance of an RDD has at least two *methods* corresponding to the Map-Reduce workflow:\n",
    "    - `map`\n",
    "    - `reduceByKey`\n",
    "    \n",
    "- These methods work in the same way as the corresponding functions we defined earlier to work with the standard Python collections.  \n",
    "\n",
    "- There are also additional RDD methods in the Apache Spark API;\n",
    "    - Apache Spark is a *super-set* of Map-Reduce.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word-count in Apache Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'be', 'or', 'not', 'to', 'be']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = \"to be or not to be\".split()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The `SparkContext` class\n",
    "\n",
    "- When working with Apache Spark we invoke methods on an object which is an instance of the `pyspark.context.SparkContext` context.\n",
    "\n",
    "- Typically, an instance of this object will be created automatically for you and assigned to the variable `sc`.\n",
    "\n",
    "- The `parallelize` method in `SparkContext` can be used to turn any ordinary Python collection into an RDD;\n",
    "    - normally we would create an RDD from a large file or an HBase table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate() \n",
    "sc = spark.sparkContex\n",
    "words_rdd = sc.parallelize(words)\n",
    "words_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mapping an RDD\n",
    "\n",
    "- Now when we invoke the `map` or `reduceByKey` methods on `my_rdd` we can set up a parallel processing computation across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuples_rdd = words_rdd.map(lambda x: (x, 1))\n",
    "word_tuples_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that we do not have a result yet.\n",
    "\n",
    "- The computation is not performed until we request the final result to be *collected*.\n",
    "\n",
    "- We do this by invoking the `collect()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 1), ('be', 1), ('or', 1), ('not', 1), ('to', 1), ('be', 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tuples_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reducing an RDD\n",
    "\n",
    "- However, we require additional processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_rdd = word_tuples_rdd.reduceByKey(lambda x, y: x + y)\n",
    "word_counts_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we request the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = word_counts_rdd.collect()\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lazy evaluation \n",
    "\n",
    "- It is only when we invoke `collect()` that the processing is performed on the cluster.\n",
    "\n",
    "- Invoking `collect()` will cause both the `map` and `reduceByKey` operations to be performed.\n",
    "\n",
    "- If the resulting collection is very large then this can be an expensive operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The head of an RDD\n",
    "\n",
    "- The `take` method is similar to `collect`, but only returns the first $n$ elements.\n",
    "\n",
    "- This can be very useful for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 2), ('be', 2)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The complete word-count example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"to be or not to be\".split()\n",
    "rdd = sc.parallelize(text)\n",
    "counts = rdd.map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda x, y: x + y)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional RDD transformations\n",
    "\n",
    "- Apache Spark offers many more methods for operating on collections of tuples over and above the standard Map-Reduce framework:\n",
    "\n",
    "    - Sorting: `sortByKey`, `sortBy`, `takeOrdered`\n",
    "    - Mapping: `flatMap`\n",
    "    - Filtering: `filter`\n",
    "    - Counting: `count`\n",
    "    - Set-theoretic: `intersection`, `union`\n",
    "    - Many others: [see the Transformations section of the programming guide](https://spark.apache.org/docs/latest/programming-guide.html#transformations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating an RDD from a text file\n",
    "\n",
    "- In the previous example, we created an RDD from a Python collection.\n",
    "\n",
    "- This is *not* typically how we would work with big data.\n",
    "\n",
    "- More commonly we would create an RDD corresponding to data in an\n",
    "HBase table, or an HDFS file.\n",
    "\n",
    "- The following example creates an RDD from a text file on the native filesystem (ext4);\n",
    "    - With bigger data, you would use an HDFS file, but the principle is the same.\n",
    "\n",
    "- Each element of the RDD corresponds to a single *line* of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TTGGCCATGCTGCCCACTCACCTAGAGCGCACAGCTGACACTGAGTCCTCTTCTGAACCTCATCCATGAA',\n",
       " 'CATATTTATGAAATCTTTCCTGGCCCCAAGTGGAAATGCCCCCTCATTTGGGTCCTCACTGAACCCCAGT',\n",
       " 'ACACAACTCTTTTGTACTACTCTATTATGCTGGGGTGTTTTTTTATTGTCTCACCTGATAAACCGTAAGC',\n",
       " 'CCCTTGAAGACAGCAACTCGTTTTTAAGCTCTTTATAACCCCAGAGCCTCGCACAGTACCTGGACCAGAT',\n",
       " 'TAAGGGGTACTTAACAGATGCTTAGTGAAGGAAGGAATGGATTTCTCACCTGGTTGCTTATCTTCTAGAC']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome = sc.textFile('../../../data/genome.txt')\n",
    "genome.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Genome example\n",
    "\n",
    "- We will use this RDD to calculate the frequencies of sequences of five bases, and then sort the sequences into descending order ranked by their frequency.\n",
    "\n",
    "- First we will define some functions to split the bases into sequences of a certain size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_characters(line, n=5):\n",
    "    result = ''\n",
    "    i = 0\n",
    "    for ch in line:\n",
    "        result = result + ch\n",
    "        i = i + 1\n",
    "        if (i % n) == 0:\n",
    "            yield result\n",
    "            result = ''\n",
    "\n",
    "def group_and_split(line):\n",
    "    return [sequence for sequence in group_characters(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcde', 'fghij', 'klmno']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_and_split('abcdefghijklmno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will transform the original text RDD into an RDD containing key-value pairs where the key is the sequence and the value is 1, as per the word-count example.\n",
    "\n",
    "- Notice that if we simply map each line of text, we will obtain multi-dimensional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['TTGGC',\n",
       "  'CATGC',\n",
       "  'TGCCC',\n",
       "  'ACTCA',\n",
       "  'CCTAG',\n",
       "  'AGCGC',\n",
       "  'ACAGC',\n",
       "  'TGACA',\n",
       "  'CTGAG',\n",
       "  'TCCTC',\n",
       "  'TTCTG',\n",
       "  'AACCT',\n",
       "  'CATCC',\n",
       "  'ATGAA'],\n",
       " ['CATAT',\n",
       "  'TTATG',\n",
       "  'AAATC',\n",
       "  'TTTCC',\n",
       "  'TGGCC',\n",
       "  'CCAAG',\n",
       "  'TGGAA',\n",
       "  'ATGCC',\n",
       "  'CCCTC',\n",
       "  'ATTTG',\n",
       "  'GGTCC',\n",
       "  'TCACT',\n",
       "  'GAACC',\n",
       "  'CCAGT']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome.map(group_and_split).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening an RDD using `flatMap`\n",
    "\n",
    "- We will need to flatten this data in order to turn it into a list of base-sequences.\n",
    "\n",
    "- We can use the `flatMap` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TTGGC', 'CATGC', 'TGCCC']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = genome.flatMap(group_and_split)\n",
    "sequences.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TTGGC', 587),\n",
       " ('CATGC', 647),\n",
       " ('TGCCC', 599),\n",
       " ('ACTCA', 775),\n",
       " ('TGACA', 831),\n",
       " ('TTCTG', 1257),\n",
       " ('AACCT', 726),\n",
       " ('TTATG', 819),\n",
       " ('AAATC', 996),\n",
       " ('TGGCC', 718)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = \\\n",
    "    sequences.map(\n",
    "        lambda w: (w, 1)).reduceByKey(lambda x, y: x + y)\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to rank each sequence according to its count.\n",
    "\n",
    "- Therefore the key (first element) of each tuple should be the count.\n",
    "\n",
    "- Thefefore we need to reverse the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tuple(key_value_pair):\n",
    "    return (key_value_pair[1], key_value_pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(587, 'TTGGC'),\n",
       " (647, 'CATGC'),\n",
       " (599, 'TGCCC'),\n",
       " (775, 'ACTCA'),\n",
       " (831, 'TGACA'),\n",
       " (1257, 'TTCTG'),\n",
       " (726, 'AACCT'),\n",
       " (819, 'TTATG'),\n",
       " (996, 'AAATC'),\n",
       " (718, 'TGGCC')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = counts.map(reverse_tuple)\n",
    "sequences.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting an RDD\n",
    "\n",
    "- Now we can sort the RDD in descending order of key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(37137, 'NNNNN'),\n",
       " (4653, 'AAAAA'),\n",
       " (4223, 'TTTTT'),\n",
       " (2788, 'AAAAT'),\n",
       " (2658, 'ATTTT'),\n",
       " (2283, 'AAATA'),\n",
       " (2276, 'TAAAA'),\n",
       " (2197, 'TTTTA'),\n",
       " (2196, 'TATTT'),\n",
       " (2185, 'AGAAA')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_sorted = sequences.sortByKey(False)\n",
    "top_ten_sequences = sequences_sorted.take(10)\n",
    "top_ten_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating $\\pi$ using Spark\n",
    "\n",
    "- We can estimate an approximate value for $\\pi$ using the following Monte-Carlo method:\n",
    "\n",
    "\n",
    "1.    Inscribe a circle in a square\n",
    "2.    Randomly generate points in the square\n",
    "3.    Determine the number of points in the square that are also in the circle\n",
    "4.    Let $r$ be the number of points in the circle divided by the number of points in the square, then $\\pi \\approx 4 r$.\n",
    "    \n",
    "- Note that the more points generated, the better the approximation\n",
    "\n",
    "See [this tutorial](https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesPI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is approximately 3.141\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample(p):\n",
    "    x, y = np.random.random(), np.random.random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "NUM_SAMPLES = 5000000\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).map(sample) \\\n",
    "             .reduce(lambda a, b: a + b)\n",
    "r = float(count) / float(NUM_SAMPLES)\n",
    "print(\"Pi is approximately %.3f\" % (4.0 * r))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
